{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Environment Setup ---y\n",
    "\n",
    "# Option 1: Using Anaconda\n",
    "# conda create --name tf python=3.8 anaconda \n",
    "# conda activate tf\n",
    "# pip install -r ../requirements.txt\n",
    "# Note: This installs an older version of TensorFlow since Anaconda \n",
    "# no longer maintains recent GPU packages. It still works reliably.\n",
    "\n",
    "# Option 2: Using Python venv\n",
    "# python3 -m venv .venv\n",
    "# source .venv/bin/activate\n",
    "# pip install -r ../requirements.txt\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import utils as np_utils\n",
    "from fMRINet import fmriNet8, fmriNet16, fmriNet32\n",
    "\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "\n",
    "# TensorFlow optimizers\n",
    "\n",
    "# AdamW lives in different places depending on TF version.\n",
    "try:\n",
    "    # TF ≥ 2.13\n",
    "    from tensorflow.keras.optimizers import AdamW\n",
    "except ImportError:\n",
    "    try:\n",
    "        # TF 2.11–2.12\n",
    "        from tensorflow.keras.optimizers.experimental import AdamW\n",
    "    except ImportError:\n",
    "        # TF 2.10.x (requires tensorflow-addons)\n",
    "        from tensorflow_addons.optimizers import AdamW\n",
    "\n",
    "# For compatibility with libraries such as iNNvestigate, \n",
    "# you may need to disable eager execution:\n",
    "# tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame from the pickle file /-/ this is the toy dataframe ; for the actual data; please consider dataframe.pkl and get in touch with the author.\n",
    "\n",
    "\n",
    "# #{'PVT': 0, 'VWM': 1, 'DOT': 2, 'MOD': 3, 'DYN': 4, 'rest': 5}\n",
    "# df = pd.read_pickle('dataframe.pkl')\n",
    "\n",
    "# df.head()\n",
    "\n",
    "\n",
    "#{'PVT': 0, 'VWM': 1, 'DOT': 2, 'MOD': 3, 'DYN': 4, 'rest': 5}\n",
    "df = pd.read_pickle('dataframe.pkl')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"Built with CUDA?\", tf.test.is_built_with_cuda())\n",
    "print(\"Visible GPUs:\", tf.config.list_physical_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjs = df[\"subject\"].unique()\n",
    "# np.random.shuffle(subjs) # do in-place shuffle\n",
    "\n",
    "# to work with the same train/validation splits while doing model development\n",
    "with open('subjs.pickle', 'rb') as f:\n",
    "    subjs = pickle.load(f)\n",
    "\n",
    "# pull train/valid data by taking subjects from shuffled list\n",
    "train_df = df[df['subject'].isin(subjs[0:45])]\n",
    "valid_df = df[df['subject'].isin(subjs[45:,])]\n",
    "\n",
    "# convert to numpy arrays and do reordering of data dimensions to feed into network\n",
    "train_label = np.array(train_df['Task'])\n",
    "\n",
    "train_data  = np.dstack(train_df['Time_Series_Data'])\n",
    "train_data  = np.expand_dims(train_data, axis=0)\n",
    "train_data  = np.transpose(train_data, axes=[3, 2, 1, 0]) # (batch, row, time, region)\n",
    "\n",
    "valid_label = np.array(valid_df['Task'])\n",
    "\n",
    "valid_data  = np.dstack(valid_df['Time_Series_Data'])\n",
    "valid_data  = np.expand_dims(valid_data, axis=0)\n",
    "valid_data  = np.transpose(valid_data, axes=[3, 2, 1, 0]) # (batch, row, time, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = np_utils.to_categorical(train_label)\n",
    "valid_label = np_utils.to_categorical(valid_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate class weights for training data to use at training time\n",
    "train_label_v2 = np.argmax(train_label, axis=1)\n",
    "a, b           = np.unique(train_label_v2, return_counts=True)\n",
    "weights        = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced', classes=a, y=train_label_v2)\n",
    "class_weights  = {0:weights[0], 1:weights[1], 2:weights[2], 3:weights[3], 4:weights[4], 5:weights[5]}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fmriNet8(num_classes=6, input_shape=(214, 277, 1), temporal_kernel_sec=60, fs=1.0)  # or fmriNet16(), fmriNet32()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=AdamW(weight_decay=0.0005), \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1,\n",
    "                               save_best_only=True)\n",
    "\n",
    "# simple learning rate schedule, half learning rate every 200 epochs\n",
    "# which seems to do ok for this data\n",
    "def lr_schedule(epoch):\n",
    "         return (0.001 * np.power(0.5, np.floor(epoch/200)))\n",
    "\n",
    "scheduler    = LearningRateScheduler(lr_schedule, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# without eager execution this takes much longer to train..\n",
    "fittedModel = model.fit(train_data, train_label, batch_size = 64, epochs = 200, \n",
    "                        verbose = 2, validation_data=(valid_data, valid_label),\n",
    "                        callbacks=[checkpointer], class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fittedModel.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(fittedModel.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(fittedModel.history['loss'], label='Train Loss')\n",
    "plt.plot(fittedModel.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('/tmp/checkpoint.h5')\n",
    "preds = model.predict(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "balanced_accuracy_score(np.argmax(valid_label, axis=1), np.argmax(preds, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = np.squeeze(model.layers[2].get_weights())\n",
    "\n",
    "fig = plt.subplots(2, 4, figsize=(12, 4))\n",
    "\n",
    "for i in range(1, 9):\n",
    "    plt.subplot(2, 4, i)\n",
    "    plt.plot(filters[:, i-1])\n",
    "    plt.title(f'Temporal Filter {i}')\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = np.squeeze(model.layers[4].get_weights())\n",
    "\n",
    "fig = plt.subplots(8, 4, figsize=(8, 12))\n",
    "\n",
    "i = 1\n",
    "for j in range(8):\n",
    "    for k in range(4):\n",
    "        plt.subplot(8, 4, i)\n",
    "        plt.plot(filters[:, j, k])\n",
    "        plt.title(f'T. Filter  {j+1}, S. Filter {k+1}')\n",
    "        i = i + 1\n",
    "    \n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
